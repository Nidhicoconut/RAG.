{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPZnEGZGk/m/8mxEaVSJIMA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nidhicoconut/RAG./blob/main/gradpruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "fQtkmejsyB96"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])\n",
        "])"
      ],
      "metadata": {
        "id": "3mTL_1VLrArF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYb3XW-qsyiY",
        "outputId": "616a9aac-5d05-4050-8968-77e0732895f2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOo_I3u_syk-",
        "outputId": "58576623-01ff-40f5-e3f9-a5dbc1a7f4b8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the ResNet block\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        return F.relu(out)\n",
        "\n",
        "# Define ResNet10\n",
        "class ResNet10(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ResNet10, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = BasicBlock(64, 64)\n",
        "        self.layer2 = BasicBlock(64, 128, stride=2)\n",
        "        self.layer3 = BasicBlock(128, 256, stride=2)\n",
        "        self.layer4 = BasicBlock(256, 512, stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        return self.fc(out)\n"
      ],
      "metadata": {
        "id": "cvGiH7Awsyn3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ResNet10(num_classes=10).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
      ],
      "metadata": {
        "id": "GdMSNf93syqr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in trainloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(trainloader):.4f}\")\n",
        "\n",
        "print(\"Training Finished!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_hZFZxLsyvK",
        "outputId": "ae991acc-0c9b-416e-95a2-a58c3b7d4696"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 1.3832\n",
            "Epoch [2/20], Loss: 0.9293\n",
            "Epoch [3/20], Loss: 0.7417\n",
            "Epoch [4/20], Loss: 0.6292\n",
            "Epoch [5/20], Loss: 0.5484\n",
            "Epoch [6/20], Loss: 0.4892\n",
            "Epoch [7/20], Loss: 0.4441\n",
            "Epoch [8/20], Loss: 0.4076\n",
            "Epoch [9/20], Loss: 0.3723\n",
            "Epoch [10/20], Loss: 0.3461\n",
            "Epoch [11/20], Loss: 0.2507\n",
            "Epoch [12/20], Loss: 0.2221\n",
            "Epoch [13/20], Loss: 0.2117\n",
            "Epoch [14/20], Loss: 0.1989\n",
            "Epoch [15/20], Loss: 0.1919\n",
            "Epoch [16/20], Loss: 0.1884\n",
            "Epoch [17/20], Loss: 0.1777\n",
            "Epoch [18/20], Loss: 0.1690\n",
            "Epoch [19/20], Loss: 0.1631\n",
            "Epoch [20/20], Loss: 0.1575\n",
            "Training Finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSRfR2q0uMxu",
        "outputId": "02217325-bdac-460d-b297-46716b6a8761"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 89.65%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So initially accuracy is 90.36%"
      ],
      "metadata": {
        "id": "Kj0AcXFk044w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fvcore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ri-jS-QFbA41",
        "outputId": "8dcad45f-02e2-4b7c-ca0d-508b8f240fd0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore) (1.26.4)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from fvcore) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (2.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from fvcore) (11.1.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from iopath>=0.1.7->fvcore) (4.12.2)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61396 sha256=49270be3e7636e61d5c89784fd95073807b45bad40611c1609911bf3063d2f3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/71/95/3b8fde5c65c6e4a806e0867c1651dcc71a1cb2f3430e8f355f\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=f4d98e2ee3edae1e4e31f06e00115e407bdfec871372831415bfb2b55527e1a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/5e/16/6117f8fe7e9c0c161a795e10d94645ebcf301ccbd01f66d8ec\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.1.1 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fvcore.nn import FlopCountAnalysis, parameter_count_table\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "\n",
        "# Create a dummy input tensor (batch_size=1, 3 color channels, 32x32 CIFAR10 image)\n",
        "dummy_input = torch.randn(1, 3, 32, 32).to(device)\n",
        "\n",
        "# Compute FLOPs\n",
        "flops = FlopCountAnalysis(model, dummy_input)\n",
        "print(f\"🔢 Total FLOPs: {flops.total()}\")\n",
        "\n",
        "# Compute Parameter Count\n",
        "print(f\"Model Parameters: \\n{parameter_count_table(model)}\")\n",
        "print(f\"Total Parameters: {sum(p.numel() for p in model.parameters())}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4gX1ExgbA7p",
        "outputId": "7a5d3725-6642-4167-b6d1-7a54a3082f30"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add_ encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
            "layer1.shortcut\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔢 Total FLOPs: 254178304\n",
            "Model Parameters: \n",
            "| name                   | #elements or shape   |\n",
            "|:-----------------------|:---------------------|\n",
            "| model                  | 4.9M                 |\n",
            "|  conv1                 |  1.7K                |\n",
            "|   conv1.weight         |   (64, 3, 3, 3)      |\n",
            "|  bn1                   |  0.1K                |\n",
            "|   bn1.weight           |   (64,)              |\n",
            "|   bn1.bias             |   (64,)              |\n",
            "|  layer1                |  74.0K               |\n",
            "|   layer1.conv1         |   36.9K              |\n",
            "|    layer1.conv1.weight |    (64, 64, 3, 3)    |\n",
            "|   layer1.bn1           |   0.1K               |\n",
            "|    layer1.bn1.weight   |    (64,)             |\n",
            "|    layer1.bn1.bias     |    (64,)             |\n",
            "|   layer1.conv2         |   36.9K              |\n",
            "|    layer1.conv2.weight |    (64, 64, 3, 3)    |\n",
            "|   layer1.bn2           |   0.1K               |\n",
            "|    layer1.bn2.weight   |    (64,)             |\n",
            "|    layer1.bn2.bias     |    (64,)             |\n",
            "|  layer2                |  0.2M                |\n",
            "|   layer2.conv1         |   73.7K              |\n",
            "|    layer2.conv1.weight |    (128, 64, 3, 3)   |\n",
            "|   layer2.bn1           |   0.3K               |\n",
            "|    layer2.bn1.weight   |    (128,)            |\n",
            "|    layer2.bn1.bias     |    (128,)            |\n",
            "|   layer2.conv2         |   0.1M               |\n",
            "|    layer2.conv2.weight |    (128, 128, 3, 3)  |\n",
            "|   layer2.bn2           |   0.3K               |\n",
            "|    layer2.bn2.weight   |    (128,)            |\n",
            "|    layer2.bn2.bias     |    (128,)            |\n",
            "|   layer2.shortcut      |   8.4K               |\n",
            "|    layer2.shortcut.0   |    8.2K              |\n",
            "|    layer2.shortcut.1   |    0.3K              |\n",
            "|  layer3                |  0.9M                |\n",
            "|   layer3.conv1         |   0.3M               |\n",
            "|    layer3.conv1.weight |    (256, 128, 3, 3)  |\n",
            "|   layer3.bn1           |   0.5K               |\n",
            "|    layer3.bn1.weight   |    (256,)            |\n",
            "|    layer3.bn1.bias     |    (256,)            |\n",
            "|   layer3.conv2         |   0.6M               |\n",
            "|    layer3.conv2.weight |    (256, 256, 3, 3)  |\n",
            "|   layer3.bn2           |   0.5K               |\n",
            "|    layer3.bn2.weight   |    (256,)            |\n",
            "|    layer3.bn2.bias     |    (256,)            |\n",
            "|   layer3.shortcut      |   33.3K              |\n",
            "|    layer3.shortcut.0   |    32.8K             |\n",
            "|    layer3.shortcut.1   |    0.5K              |\n",
            "|  layer4                |  3.7M                |\n",
            "|   layer4.conv1         |   1.2M               |\n",
            "|    layer4.conv1.weight |    (512, 256, 3, 3)  |\n",
            "|   layer4.bn1           |   1.0K               |\n",
            "|    layer4.bn1.weight   |    (512,)            |\n",
            "|    layer4.bn1.bias     |    (512,)            |\n",
            "|   layer4.conv2         |   2.4M               |\n",
            "|    layer4.conv2.weight |    (512, 512, 3, 3)  |\n",
            "|   layer4.bn2           |   1.0K               |\n",
            "|    layer4.bn2.weight   |    (512,)            |\n",
            "|    layer4.bn2.bias     |    (512,)            |\n",
            "|   layer4.shortcut      |   0.1M               |\n",
            "|    layer4.shortcut.0   |    0.1M              |\n",
            "|    layer4.shortcut.1   |    1.0K              |\n",
            "|  fc                    |  5.1K                |\n",
            "|   fc.weight            |   (10, 512)          |\n",
            "|   fc.bias              |   (10,)              |\n",
            "Total Parameters: 4903242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. Accuracy: 90.36%\n",
        "2. FLOPs: 254178304\n",
        "3. Total Parameters: 4903242\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Jo5PBbeWboux"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SBVUsUeXbA-g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}